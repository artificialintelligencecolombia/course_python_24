==============================
ANACONDA
==============================

---------------------
Installation
---------------------

1. Search for 'anaconda distribution' in google.
2. Donwload it.
3. Open it, click next.

NOTE:
> In order to avoid issues, we've to create the 'anaconda' folder in C:\
    So the destination folder is C:\anaconda\

4. Click ok -> Click next.

---------------------
List of commands
---------------------

Run the Anaconda Prompt shell.

> For managing venvs is not required to run anaconda as admin -> It is required for installng packages.

1) conda --version
to ensure conda is installed correctly

2) conda create --name myenv
create a new venv

3) conda create --name myenv python=3.11
specify a python version

4) conda activate myenv
activate venv

5) conda install package_name
install any package with venv activated

6) conda install package_name=1.2.3
install a specific version of a package

7) conda list
list installed packages

8) conda deactivate
deactivate the venv

9) conda env list
list all venvs created in anaconda

10) conda remove --name myenv --all
delete a specific venv

11) conda update conda
update conda itself

12) conda update --all
update all packages in a venv. FIRST ACTIVATE THE venv.

TIPS:
> Ensure that the packages and python versions are compatible to avoid dependencies conflicts.

Backup
-----------------

conda list --explicit > myenv.txt

- list all the packages installed in the active venv. '--explicit' flag includes the precise versions of the packages.
- '>' redirects the output to 'myenv.txt' file where the list will be saved.

conda create --name <env_name> --file myenv.txt

- create a new venv with a specified name. '--file myenv.txt' use the file created to install the exact packages listed in that file into the new venv.

---------------------
pyspark-env 
---------------------

> conda activate spark-env

> conda install -c conda-forge pyspark
- 'c conda-forge' option specifies the Conda Forge channel, which ofthe has the lastest version of packages.

> pyspark --version 

Note:
We can start pyspark shell with 'pyspark' or start Jupyter and create a new notebook using pyspark.