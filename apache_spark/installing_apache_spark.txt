==============================
INSTALLATION PROCESS
==============================

---------------------
Prerequisites
---------------------

1. Java development kit
2. Python > 3.8
3. Spark + Hadoop 

---------------------
Step By Step
---------------------

JDK
----------------

1. Download newest version of JDK (installer)
2. Click on the installer -> change the directory of installation.
3. Go to C/. -> create 'java' folder in here.
4. Go to new 'java' folder. 
5. Create 'jdk' folder -> Select this folder and click in ok.
6. Click NEXT and thats it.

Python 
----------------
1. Download and install python.

Spark
----------------
1. Go to spark website and choose the package type:
    Pre-built for Apache Hadoop 3.3 and later (8/23/2024)
2. Download the tar file.
3. Go C/ create a new folder 'spark'. Click on it. Paste the tar file from the download on here. Extract the file.

Download win utils
----------------
1. go to https://github.com/steveloughran/winutils
2. Pick the hadoop installed version (3.0) -> go to bin directory -> Click on winutils.exe and download it.
3. Go to C/ again and create a folder 'hadoop' -> Click on it -> Create 'bin' directory -> click on it. 
4. Paste the winutil.exe file on here.
5. Hadoop is setted up.

Test
----------------
1. Open cmd 
2. - java -version
3. - python --version

Set up PATHs for Spark and Hadoop
----------------
1. Go to windows -> Environment variables.
2. On variables de usuario, click on new.
    variable name: JAVA_HOME
    variable value: C/java/jdk
3. Click ok.

4. On variables de usuario, click on new.
    variable name: HADOOP_HOME
    variable value: C/hadoop
5. Click ok.
6. On variables de usuario, click on new.
    variable name: SPARK_HOME
    variable value: C/spark/SPARK 3.5.2-bin-hadoop3
5. Click ok.
6. On variables de usuario, click on new.
    variable name: PYSPARK_HOME
    variable value: C/Users/username/AppData/Local/Programs/Python/Python3.12
7. Click ok.
8. ADD TO THE variable value: 
    C/Users/username/AppData/Local/Programs/Python/Python3.12/python.exe

9. Go to System Variables -> Path -> Edit.
10. Click on new -> %JAVA_HOME%\bin
11. Click on new -> %HADOOP_HOME%\bin
12. Click on new -> %SPARK_HOME%\bin
13. ok -> ok -> thats it.

Test
----------------
1. Open cmd 
2. - spark-shell
3. Verify that all shit appears (with scala)
4. quit()
5. - pyspark
6. print(pyspark.version)